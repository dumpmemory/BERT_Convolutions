# EDITED to read training file incrementally for tokenization.
# Modified with a new IterableTextDataset class, allowing iterable training data.
import codecs
import os
import pickle
import random
import time
from typing import Dict, List, Optional

import torch
from torch.utils.data.dataset import Dataset, IterableDataset

from filelock import FileLock

from ...tokenization_utils import PreTrainedTokenizer
from ...utils import logging


logger = logging.get_logger(__name__)


# Text dataset that iterates through a file.
# Iterable file can be obtained by using the saved examples generated by
# LineByLineTextDataset, converting into a text file.
class IterableTextDataset(IterableDataset):
    """
    Iterable version of TextDataset. Data must be preshuffled.
    Each line in the input file should be a string of integers separated by spaces.
    [CLS] and [SEP] tokens should already be included.
    Each line should correspond to one example (one or two sentences), but
    padding and truncation is handled automatically.
    """

    # This is the iterator that is returned by the iter() method.
    class ExampleIterator:
        def __init__(self, file_path: str, block_size: int, pad_token_id: int, sep_token_id: int,
                     use_sop_head: bool):
            self.file_path = file_path
            # Start the input file from the beginning.
            self.input_file = codecs.open(file_path, 'rb', encoding='utf-8')
            self.block_size = block_size
            self.pad_token_id = pad_token_id
            self.use_sop_head = use_sop_head
            if use_sop_head:
                self.sep_token_id = sep_token_id

        def __iter__(self):
            return self

        def __next__(self):
            return self._next_example()

        def _next_example(self):
            # In case this is called after the input file was closed.
            if self.input_file == None:
                print('No input file (or input file has been closed).')
                raise StopIteration
            # Try to read a string of space-separated integers.
            # Each example should be: [CLS] sent_1 [SEP] sent_2 [SEP]
            example_string = self.input_file.readline()
            while example_string == '\n': # Skip new lines.
                example_string = self.input_file.readline()
            if example_string == '': # This only occurs at the end of a file (otherwise there would at least be a newline character).
                self.input_file.close()
                print('Example iterator complete.')
                self.input_file = None
                raise StopIteration
            example_string = example_string.strip()
            example = [int(token_id) for token_id in example_string.split()]
            # Possibly reverse the order of the two sentences.
            if self.use_sop_head:
                sop_switched = random.random() > 0.5
                if sop_switched:
                    first_sentence_end_idx = example.index(self.sep_token_id)
                    sent_1 = example[1:first_sentence_end_idx] # Do not include cls_token.
                    sent_2 = example[first_sentence_end_idx+1:-1] # Do not include last sep_token.
                    switched_example = [example[0]] + sent_2 +  [self.sep_token_id] + sent_1 + [self.sep_token_id]
                    example = switched_example
            # Truncating and padding is done here.
            if len(example) > self.block_size:
                example = example[0:self.block_size]
            else:
                diff = self.block_size-len(example)
                example.extend([self.pad_token_id]*diff)
            if self.use_sop_head:
                return {
                    "input_ids": torch.tensor(example, dtype=torch.long),
                    "sentence_order_label": torch.tensor(sop_switched, dtype=torch.long),
                }
            else:
                return example

    # Init IterableTextDataset.
    def __init__(
        self,
        file_path: str,
        block_size: int,
        pad_token_id: int,
        sep_token_id: int, # Only used for SOP.
        use_sop_head: bool = False,
    ):
        super(IterableTextDataset).__init__()
        assert os.path.isfile(file_path), f"Input file path {file_path} not found"
        self.input_filepath = file_path
        self.block_size = block_size
        self.pad_token_id = pad_token_id
        self.use_sop_head = use_sop_head
        self.sep_token_id = sep_token_id # Only used for SOP head to switch sentence orders.

        # Initially get total num_examples.
        example_count = 0
        infile = codecs.open(file_path, 'rb', encoding='utf-8')
        for line in infile:
            example_count += 1
        self.num_examples = example_count
        infile.close()

    def __iter__(self):
        return self.ExampleIterator(self.input_filepath, self.block_size, self.pad_token_id, self.sep_token_id,
                                    self.use_sop_head)

    def __len__(self):
        return self.num_examples


# Edited to read training file incrementally, and to read from a certain line number.
class TextDataset(Dataset):
    """
    This will be superseded by a framework-agnostic approach
    soon.
    """

    def __init__(
        self,
        tokenizer: PreTrainedTokenizer,
        file_path: str,
        block_size: int,
        overwrite_cache=False,
        cache_dir: Optional[str] = None,
        per_line: bool = False
    ):
        assert os.path.isfile(file_path), f"Input file path {file_path} not found"

        block_size = block_size - tokenizer.num_special_tokens_to_add(pair=False)

        directory, filename = os.path.split(file_path)
        cached_features_file = os.path.join(
            cache_dir if cache_dir is not None else directory,
            "cached_lm_{}_{}_{}".format(
                tokenizer.__class__.__name__,
                str(block_size),
                filename,
            ),
        )

        # Make sure only the first process in distributed training processes the dataset,
        # and the others will use the cache.
        lock_path = cached_features_file + ".lock"
        with FileLock(lock_path):

            start_line = 0
            max_stored_line_count = 10000
            save_every = 1000000 # Save a copy of the examples every n lines.

            # Just use existing cached features.
            if os.path.exists(cached_features_file) and start_line == 0 and not overwrite_cache:
                start = time.time()
                with open(cached_features_file, "rb") as handle:
                    self.examples = pickle.load(handle)
                logger.info(
                    f"Loading features from cached file {cached_features_file} [took %.3f s]", time.time() - start
                )

            else:
                logger.info(f"Creating features from dataset file at {directory}")

                # Load existing features file.
                self.examples = []
                if start_line != 0:
                    with open(cached_features_file, "rb") as handle:
                        self.examples = pickle.load(handle)

                textfile = open(file_path, encoding="utf-8")
                stored_text = ''
                total_line_count = 0
                for line in textfile:
                    total_line_count += 1
                    if total_line_count <= start_line:
                        continue
                    stored_text = stored_text + line
                    if total_line_count % max_stored_line_count == 0:
                        print("Tokenizing lines.")
                        # Process the stored lines.
                        tokenized_text = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(stored_text))
                        print("Building examples.")
                        for i in range(0, len(tokenized_text) - block_size + 1, block_size):  # Truncate in block of block_size
                            self.examples.append(
                                tokenizer.build_inputs_with_special_tokens(tokenized_text[i : i + block_size])
                            )
                        # Reset for the next set of lines.
                        stored_text = ''
                        print("Reading more lines starting at line {}.".format(total_line_count))
                    if total_line_count % save_every == 0:
                        # Save a copy of the examples so far.
                        print("Saving a copy of {} examples.".format(len(self.examples)))
                        with open(cached_features_file, "wb") as handle:
                            pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)
                textfile.close()
                # Process the remaining set of lines.
                tokenized_text = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(stored_text))
                for i in range(0, len(tokenized_text) - block_size + 1, block_size):  # Truncate in block of block_size
                    self.examples.append(
                        tokenizer.build_inputs_with_special_tokens(tokenized_text[i : i + block_size])
                    )

                # Note that we are losing the last truncated example here for the sake of simplicity (no padding)
                # If your dataset is small, first you should loook for a bigger one :-) and second you
                # can change this behavior by adding (model specific) padding.

                start = time.time()
                with open(cached_features_file, "wb") as handle:
                    pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)
                logger.info(
                    "Saving features into cached file %s [took %.3f s]", cached_features_file, time.time() - start
                )

    def __len__(self):
        return len(self.examples)

    def __getitem__(self, i) -> torch.Tensor:
        return torch.tensor(self.examples[i], dtype=torch.long)


# Edited to process incrementally, combine every two sentences, and cache the created examples.
# The cached examples are unpadded and untruncated. They are padded/truncated on the fly.
class LineByLineTextDataset(Dataset):
    """
    This will be superseded by a framework-agnostic approach
    soon.
    """

    def __init__(self, tokenizer: PreTrainedTokenizer, file_path: str, block_size: int):
        assert os.path.isfile(file_path), f"Input file path {file_path} not found"
        directory, filename = os.path.split(file_path)
        cached_features_file = os.path.join(
            directory,
            "cached_lm_line_pairs_{}_{}_{}".format(
                tokenizer.__class__.__name__,
                str(block_size),
                filename,
            ),
        )

        self.cls_token_id = tokenizer.cls_token_id
        self.sep_token_id = tokenizer.sep_token_id
        self.pad_token_id = tokenizer.pad_token_id
        self.block_size = block_size

        # Make sure only the first process in distributed training processes the dataset,
        # and the others will use the cache.
        lock_path = cached_features_file + ".lock"
        with FileLock(lock_path):

            start_line = 0
            max_stored_line_count = 100000 # Ideally, should be an even number so that sentences are paired together.
            save_every = 1000000 # Save a copy of the examples every n lines.

            logger.info(f"Creating features from dataset file at {directory}")

            # Load existing features file.
            self.examples = []
            if start_line != 0:
                with open(cached_features_file, "rb") as handle:
                    self.examples = pickle.load(handle)

            textfile = open(file_path, encoding="utf-8")
            total_line_count = 0
            stored_lines = []
            for line in textfile:
                total_line_count += 1
                if total_line_count <= start_line:
                    continue
                stripped_line = line.strip()
                if stripped_line != '':
                    stored_lines.append(stripped_line)
                # Process the currently stored lines.
                if total_line_count % max_stored_line_count == 0:
                    print("Processing {} lines.".format(max_stored_line_count))
                    batch_encoding = tokenizer(stored_lines, add_special_tokens=False, truncation=True, max_length=99999)
                    for i in range(0, len(batch_encoding["input_ids"])-1, 2): # Subtract one from the length so that the last unpaired example is skipped.
                        sent_1 = batch_encoding["input_ids"][i]
                        sent_2 = batch_encoding["input_ids"][i+1]
                        example = [self.cls_token_id] + sent_1 +  [self.sep_token_id] + sent_2 + [self.sep_token_id]
                        # Note that these examples are unpadded and un-truncated.
                        self.examples.append(example)
                    stored_lines = []
                if total_line_count % save_every == 0:
                    # Save a copy of the examples so far.
                    print("Saving a copy of {} examples.".format(len(self.examples)))
                    with open(cached_features_file, "wb") as handle:
                        pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)
            textfile.close()
            # Process the remaining set of lines. This is copied from above for maximal bad code style!
            batch_encoding = tokenizer(stored_lines, add_special_tokens=False, truncation=True, max_length=99999)
            for i in range(0, len(batch_encoding["input_ids"])-1, 2):
                sent_1 = batch_encoding["input_ids"][i]
                sent_2 = batch_encoding["input_ids"][i+1]
                example = [self.cls_token_id] + sent_1 +  [self.sep_token_id] + sent_2 + [self.sep_token_id]
                self.examples.append(example)

            # Cached, but this is just in case it is needed later (e.g. to convert the cached version into an iterable).
            print("Saving a copy of {} examples.".format(len(self.examples)))
            with open(cached_features_file, "wb") as handle:
                pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)

    def __len__(self):
        return len(self.examples)

    def __getitem__(self, i) -> torch.Tensor:
        # Truncate or pad on the fly.
        example = self.examples[i]
        if len(example) > self.block_size:
            example = example[0:self.block_size]
        else:
            diff = self.block_size-len(example)
            example.extend([self.pad_token_id]*diff)
        return {
            "input_ids": torch.tensor(example, dtype=torch.long),
            "sentence_order_label": torch.tensor(0, dtype=torch.long) # Just don't switch any orders (don't use this for actually testing SOP predictions!).
        }


# Not modified.
class LineByLineWithSOPTextDataset(Dataset):
    """
    Dataset for sentence order prediction task, prepare sentence pairs for SOP task
    """

    def __init__(self, tokenizer: PreTrainedTokenizer, file_dir: str, block_size: int):
        assert os.path.isdir(file_dir)
        logger.info(f"Creating features from dataset file folder at {file_dir}")
        self.examples = []
        # TODO: randomness could apply a random seed, ex. rng = random.Random(random_seed)
        # file path looks like ./dataset/wiki_1, ./dataset/wiki_2
        for file_name in os.listdir(file_dir):
            file_path = os.path.join(file_dir, file_name)
            assert os.path.isfile(file_path)
            article_open = False
            with open(file_path, encoding="utf-8") as f:
                original_lines = f.readlines()
                article_lines = []
                for line in original_lines:
                    if "<doc id=" in line:
                        article_open = True
                    elif "</doc>" in line:
                        article_open = False
                        document = [
                            tokenizer.convert_tokens_to_ids(tokenizer.tokenize(line))
                            for line in article_lines[1:]
                            if (len(line) > 0 and not line.isspace())
                        ]

                        examples = self.create_examples_from_document(document, block_size, tokenizer)
                        self.examples.extend(examples)
                        article_lines = []
                    else:
                        if article_open:
                            article_lines.append(line)

        logger.info("Dataset parse finished.")

    def create_examples_from_document(self, document, block_size, tokenizer, short_seq_prob=0.1):
        """Creates examples for a single document."""

        # Account for special tokens
        max_num_tokens = block_size - tokenizer.num_special_tokens_to_add(pair=True)

        # We *usually* want to fill up the entire sequence since we are padding
        # to `block_size` anyways, so short sequences are generally wasted
        # computation. However, we *sometimes*
        # (i.e., short_seq_prob == 0.1 == 10% of the time) want to use shorter
        # sequences to minimize the mismatch between pre-training and fine-tuning.
        # The `target_seq_length` is just a rough target however, whereas
        # `block_size` is a hard limit.
        target_seq_length = max_num_tokens
        if random.random() < short_seq_prob:
            target_seq_length = random.randint(2, max_num_tokens)

        # We DON'T just concatenate all of the tokens from a document into a long
        # sequence and choose an arbitrary split point because this would make the
        # next sentence prediction task too easy. Instead, we split the input into
        # segments "A" and "B" based on the actual "sentences" provided by the user
        # input.
        examples = []
        current_chunk = []  # a buffer stored current working segments
        current_length = 0
        i = 0
        while i < len(document):
            segment = document[i]  # get a segment
            if not segment:
                i += 1
                continue
            current_chunk.append(segment)  # add a segment to current chunk
            current_length += len(segment)  # overall token length
            # if current length goes to the target length or reaches the end of file, start building token a and b
            if i == len(document) - 1 or current_length >= target_seq_length:
                if current_chunk:
                    # `a_end` is how many segments from `current_chunk` go into the `A` (first) sentence.
                    a_end = 1
                    # if current chunk has more than 2 sentences, pick part of it `A` (first) sentence
                    if len(current_chunk) >= 2:
                        a_end = random.randint(1, len(current_chunk) - 1)
                    # token a
                    tokens_a = []
                    for j in range(a_end):
                        tokens_a.extend(current_chunk[j])

                    # token b
                    tokens_b = []
                    for j in range(a_end, len(current_chunk)):
                        tokens_b.extend(current_chunk[j])

                    if len(tokens_a) == 0 or len(tokens_b) == 0:
                        continue

                    # switch tokens_a and tokens_b randomly
                    if random.random() < 0.5:
                        is_next = False
                        tokens_a, tokens_b = tokens_b, tokens_a
                    else:
                        is_next = True

                    def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens):
                        """Truncates a pair of sequences to a maximum sequence length."""
                        while True:
                            total_length = len(tokens_a) + len(tokens_b)
                            if total_length <= max_num_tokens:
                                break
                            trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b
                            assert len(trunc_tokens) >= 1
                            # We want to sometimes truncate from the front and sometimes from the
                            # back to add more randomness and avoid biases.
                            if random.random() < 0.5:
                                del trunc_tokens[0]
                            else:
                                trunc_tokens.pop()

                    truncate_seq_pair(tokens_a, tokens_b, max_num_tokens)
                    assert len(tokens_a) >= 1
                    assert len(tokens_b) >= 1

                    # add special tokens
                    input_ids = tokenizer.build_inputs_with_special_tokens(tokens_a, tokens_b)
                    # add token type ids, 0 for sentence a, 1 for sentence b
                    token_type_ids = tokenizer.create_token_type_ids_from_sequences(tokens_a, tokens_b)

                    example = {
                        "input_ids": torch.tensor(input_ids, dtype=torch.long),
                        "token_type_ids": torch.tensor(token_type_ids, dtype=torch.long),
                        "sentence_order_label": torch.tensor(0 if is_next else 1, dtype=torch.long),
                    }
                    examples.append(example)
                current_chunk = []  # clear current chunk
                current_length = 0  # reset current text length
            i += 1  # go to next line
        return examples

    def __len__(self):
        return len(self.examples)

    def __getitem__(self, i) -> Dict[str, torch.tensor]:
        return self.examples[i]


class TextDatasetForNextSentencePrediction(Dataset):
    """
    This will be superseded by a framework-agnostic approach
    soon.
    """

    def __init__(
        self,
        tokenizer: PreTrainedTokenizer,
        file_path: str,
        block_size: int,
        overwrite_cache=False,
        short_seq_probability=0.1,
        nsp_probability=0.5,
    ):
        assert os.path.isfile(file_path), f"Input file path {file_path} not found"

        self.block_size = block_size - tokenizer.num_special_tokens_to_add(pair=True)
        self.short_seq_probability = short_seq_probability
        self.nsp_probability = nsp_probability

        directory, filename = os.path.split(file_path)
        cached_features_file = os.path.join(
            directory,
            "cached_nsp_{}_{}_{}".format(
                tokenizer.__class__.__name__,
                str(block_size),
                filename,
            ),
        )

        self.tokenizer = tokenizer

        # Make sure only the first process in distributed training processes the dataset,
        # and the others will use the cache.
        lock_path = cached_features_file + ".lock"

        # Input file format:
        # (1) One sentence per line. These should ideally be actual sentences, not
        # entire paragraphs or arbitrary spans of text. (Because we use the
        # sentence boundaries for the "next sentence prediction" task).
        # (2) Blank lines between documents. Document boundaries are needed so
        # that the "next sentence prediction" task doesn't span between documents.
        #
        # Example:
        # I am very happy.
        # Here is the second sentence.
        #
        # A new document.

        with FileLock(lock_path):
            if os.path.exists(cached_features_file) and not overwrite_cache:
                start = time.time()
                with open(cached_features_file, "rb") as handle:
                    self.examples = pickle.load(handle)
                logger.info(
                    f"Loading features from cached file {cached_features_file} [took %.3f s]", time.time() - start
                )
            else:
                logger.info(f"Creating features from dataset file at {directory}")

                self.documents = [[]]
                with open(file_path, encoding="utf-8") as f:
                    while True:
                        line = f.readline()
                        if not line:
                            break
                        line = line.strip()

                        # Empty lines are used as document delimiters
                        if not line and len(self.documents[-1]) != 0:
                            self.documents.append([])
                        tokens = tokenizer.tokenize(line)
                        tokens = tokenizer.convert_tokens_to_ids(tokens)
                        if tokens:
                            self.documents[-1].append(tokens)

                logger.info(f"Creating examples from {len(self.documents)} documents.")
                self.examples = []
                for doc_index, document in enumerate(self.documents):
                    self.create_examples_from_document(document, doc_index)

                start = time.time()
                with open(cached_features_file, "wb") as handle:
                    pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)
                logger.info(
                    "Saving features into cached file %s [took %.3f s]", cached_features_file, time.time() - start
                )

    def create_examples_from_document(self, document: List[List[int]], doc_index: int):
        """Creates examples for a single document."""

        max_num_tokens = self.block_size - self.tokenizer.num_special_tokens_to_add(pair=True)

        # We *usually* want to fill up the entire sequence since we are padding
        # to `block_size` anyways, so short sequences are generally wasted
        # computation. However, we *sometimes*
        # (i.e., short_seq_prob == 0.1 == 10% of the time) want to use shorter
        # sequences to minimize the mismatch between pre-training and fine-tuning.
        # The `target_seq_length` is just a rough target however, whereas
        # `block_size` is a hard limit.
        target_seq_length = max_num_tokens
        if random.random() < self.short_seq_probability:
            target_seq_length = random.randint(2, max_num_tokens)

        current_chunk = []  # a buffer stored current working segments
        current_length = 0
        i = 0

        while i < len(document):
            segment = document[i]
            current_chunk.append(segment)
            current_length += len(segment)
            if i == len(document) - 1 or current_length >= target_seq_length:
                if current_chunk:
                    # `a_end` is how many segments from `current_chunk` go into the `A`
                    # (first) sentence.
                    a_end = 1
                    if len(current_chunk) >= 2:
                        a_end = random.randint(1, len(current_chunk) - 1)

                    tokens_a = []
                    for j in range(a_end):
                        tokens_a.extend(current_chunk[j])

                    tokens_b = []

                    if len(current_chunk) == 1 or random.random() < self.nsp_probability:
                        is_random_next = True
                        target_b_length = target_seq_length - len(tokens_a)

                        # This should rarely go for more than one iteration for large
                        # corpora. However, just to be careful, we try to make sure that
                        # the random document is not the same as the document
                        # we're processing.
                        for _ in range(10):
                            random_document_index = random.randint(0, len(self.documents) - 1)
                            if random_document_index != doc_index:
                                break

                        random_document = self.documents[random_document_index]
                        random_start = random.randint(0, len(random_document) - 1)
                        for j in range(random_start, len(random_document)):
                            tokens_b.extend(random_document[j])
                            if len(tokens_b) >= target_b_length:
                                break
                        # We didn't actually use these segments so we "put them back" so
                        # they don't go to waste.
                        num_unused_segments = len(current_chunk) - a_end
                        i -= num_unused_segments
                    # Actual next
                    else:
                        is_random_next = False
                        for j in range(a_end, len(current_chunk)):
                            tokens_b.extend(current_chunk[j])

                    assert len(tokens_a) >= 1
                    assert len(tokens_b) >= 1

                    self.examples.append(
                        {"tokens_a": tokens_a, "tokens_b": tokens_b, "is_random_next": is_random_next}
                    )

                current_chunk = []
                current_length = 0

            i += 1

    def __len__(self):
        return len(self.examples)

    def __getitem__(self, i):
        return self.examples[i]
